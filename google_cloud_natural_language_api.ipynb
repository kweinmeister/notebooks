{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nl_apis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "environment": {
      "name": "tf2-gpu.2-1.m46",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjqrNbBsyNIF"
      },
      "source": [
        "# How to use the Google Cloud Natural Language API\n",
        "## Clothing Reviews Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1rudE58yNIG"
      },
      "source": [
        "This notebook demonstrates how to use the [Google Cloud Natural Language API](https://cloud.google.com/natural-language/docs) for:\n",
        "* Sentiment analysis\n",
        "* Entity extraction\n",
        "* Syntax analysis\n",
        "* Text classification\n",
        "\n",
        "This notebook will also show how to visualize results from the API with the [Seaborn data visualization library](https://seaborn.pydata.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3OZ4xrcziH5"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdawvuqM2lVO"
      },
      "source": [
        "### Upload dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjfHit6ezEPy"
      },
      "source": [
        "The dataset we will use is [Kaggle - Women's E-Commerce Clothing Reviews](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews).\n",
        "\n",
        "It is necessary to download the CSV from Kaggle, and upload it into the same directory as this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nkKj1tVOvR3"
      },
      "source": [
        "# [Colab Only] Upload CSV programatically\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:    \n",
        "  from google.colab import files\n",
        "  files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b8UlplJ35Zy"
      },
      "source": [
        "### [Colab Only] Create a GCP project and perform setup tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDQGRufi36Ph"
      },
      "source": [
        "Follow these steps to **Setup a Project** in the [documentation](https://cloud.google.com/natural-language/docs/quickstart-client-libraries).\n",
        "\n",
        "Create a new key as JSON, and download it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dUjwSMu2vxe"
      },
      "source": [
        "### [Colab Only] Authenticate with GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfskCY45JGaH"
      },
      "source": [
        "# Upload the downloaded JSON file that contains your key.\n",
        "\n",
        "if 'google.colab' in sys.modules:    \n",
        "  from google.colab import files\n",
        "  keyfile_upload = files.upload()\n",
        "  keyfile = list(keyfile_upload.keys())[0]\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS $keyfile\n",
        "  ! gcloud auth activate-service-account --key-file $keyfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OHizJAPyNIG"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvuIUlb9yNIG"
      },
      "source": [
        "# Load data from CSV\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjK7EHQSyNIG"
      },
      "source": [
        "# Filter dataset to rows with reviews > 100 chars long\n",
        "\n",
        "df = df.loc[df['Review Text'].str.len() > 100]\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USPjBFbkyNIH"
      },
      "source": [
        "# Pick one of the reviews as an example\n",
        "\n",
        "text = df['Review Text'][2]\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A-VSiRUyNIH"
      },
      "source": [
        "### Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao7yvS7UAQEP"
      },
      "source": [
        "We'll now instantiate the Natural Language API client, and invoke the sentiment analysis function on our text. The sentiment score and magnitude will be returned.\n",
        "\n",
        "The score of a document's sentiment indicates the overall emotion of a document. The magnitude of a document's sentiment indicates how much emotional content is present within the document, and this value is often proportional to the length of the document. See the [documentation](https://cloud.google.com/natural-language/docs/basics#sentiment-analysis-values) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86WqQH-NyNIH"
      },
      "source": [
        "# Imports the Google Cloud client library\n",
        "from google.cloud import language\n",
        "from google.cloud.language import enums\n",
        "from google.cloud.language import types\n",
        "\n",
        "# Instantiates a client\n",
        "client = language.LanguageServiceClient()\n",
        "\n",
        "# The text to analyze\n",
        "document = types.Document(\n",
        "    content=text,\n",
        "    type=enums.Document.Type.PLAIN_TEXT)\n",
        "\n",
        "# Detects the sentiment of the text\n",
        "sentiment = client.analyze_sentiment(document).document_sentiment\n",
        "\n",
        "print('Text: {}\\n'.format(text))\n",
        "print('Sentiment: {}, {}'.format(round(sentiment.score, 2), round(sentiment.magnitude, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNq2GvHXyNIH"
      },
      "source": [
        "# Take a sample of the reviews for analysis\n",
        "SAMPLE_SIZE = 100\n",
        "df_sample = df.sample(SAMPLE_SIZE).copy().reset_index()\n",
        "scores, magnitudes = list(), list()\n",
        "\n",
        "\n",
        "# Iterate through each sample and invoke the API\n",
        "for review in df_sample['Review Text']:\n",
        "    document = types.Document(content=review, type=enums.Document.Type.PLAIN_TEXT)\n",
        "    sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
        "    scores.append(sentiment.score)\n",
        "    magnitudes.append(sentiment.magnitude)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0ERE4S4yNIH"
      },
      "source": [
        "# Merge the scores & magnitudes returned from the API with the original records\n",
        "\n",
        "df_sample['Score'] = scores\n",
        "df_sample['Magnitude'] = magnitudes\n",
        "df_sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BiFP7JUyNIH"
      },
      "source": [
        "# Plot the sentiment for each clothing category\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "\n",
        "_ = sns.boxplot(x=\"Class Name\", y=\"Score\", data=df_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ad7hwHdyNIH"
      },
      "source": [
        "### Entity extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S3ImH80yNIH"
      },
      "source": [
        "# Print the text we want to extract entities from\n",
        "\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbQKqUpayNIH"
      },
      "source": [
        "# Analyze entities\n",
        "\n",
        "document = {\"content\": text, \"type\": enums.Document.Type.PLAIN_TEXT}\n",
        "\n",
        "response = client.analyze_entities(document, encoding_type=enums.EncodingType.UTF8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32HLa_wdDFLa"
      },
      "source": [
        "Next, we will create a regular expression that will highlight every entity found in the text by wrapping the entity with an escape sequence.\n",
        "\n",
        "By the way, the API also returns the position of each entity, in case you prefer to use a different approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkiD5sXXyNIH"
      },
      "source": [
        "# Get list of entity names from returned entities (e.g. ['hopes', 'dress', ...])\n",
        "import re\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "\n",
        "entities = MessageToDict(response)['entities']\n",
        "for elem in entities:\n",
        "    entity_names = [elem['name'] for elem in entities]\n",
        "\n",
        "# Create a regular expression pattern to match any of the entity names\n",
        "pattern = '(' + '|'.join(entity_names) + ')'\n",
        "pattern\n",
        "\n",
        "HIGHLIGHT = '\\x1b[1;31m' # ANSI escape code sequence for red\n",
        "RESET = '\\x1b[0m'\n",
        "\n",
        "# Print out the text with the entities highlighted\n",
        "highlighted_text = re.sub(re.compile(pattern), HIGHLIGHT + '\\\\1' + RESET, text)\n",
        "print(highlighted_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2LFu5VByNIH"
      },
      "source": [
        "### Syntax analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC1l1F-6yNIH"
      },
      "source": [
        "# Analyze syntax\n",
        "\n",
        "syntax = client.analyze_syntax(document = types.Document(content=text, type=enums.Document.Type.PLAIN_TEXT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi8HKBU9yNIH"
      },
      "source": [
        "# Count the number of times each part of speech occurs in the text\n",
        "\n",
        "# Create a list of all possible parts of speech\n",
        "all_tags = [e.name for e in enums.PartOfSpeech.Tag]\n",
        "\n",
        "# Create dictionary for each part-of-speech\n",
        "tag_counts = dict.fromkeys(all_tags, 0)\n",
        "\n",
        "# Review each token and add to the counter\n",
        "for token in syntax.tokens:\n",
        "    part_of_speech = token.part_of_speech\n",
        "    tag = enums.PartOfSpeech.Tag(part_of_speech.tag).name\n",
        "    tag_counts[tag] += 1\n",
        "\n",
        "# Sort the counts in descending order, and plot them\n",
        "sorted_counts = dict(sorted(tag_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "_ = sns.barplot(x=list(sorted_counts.values()), y=list(sorted_counts.keys()), )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnohxnbWyNIH"
      },
      "source": [
        "# Analyze the parts of speech for each review in the sample. This time, calculate the % by part-of-speech (e.g. 20% Noun, 15% Adjective, etc.)\n",
        "\n",
        "tags = list()\n",
        "\n",
        "for review in df_sample['Review Text']:\n",
        "    document = types.Document(content=review, type=enums.Document.Type.PLAIN_TEXT)\n",
        "    syntax = client.analyze_syntax(document)\n",
        "    \n",
        "    tag_ratios = dict.fromkeys(all_tags, 0)\n",
        "    for token in syntax.tokens:\n",
        "        \n",
        "        part_of_speech = token.part_of_speech\n",
        "        tag = enums.PartOfSpeech.Tag(part_of_speech.tag).name\n",
        "        tag_ratios[tag] += 1 / len(syntax.tokens)\n",
        "    tags.append(tag_ratios)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCS7NcY_yNIH"
      },
      "source": [
        "# Append the parts of speech to the review dataframe\n",
        "\n",
        "df_sample = pd.concat([df_sample, pd.DataFrame(tags, columns=all_tags)], axis=1)\n",
        "\n",
        "df_sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGGC4bwOyNII"
      },
      "source": [
        "# Let's see if there is any correlation between sentiment and a couple common parts of speech\n",
        "\n",
        "sns.lmplot(x='ADJ', y='Score', data=df_sample)\n",
        "sns.lmplot(x='VERB', y='Score', data=df_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-y5S-HNyNII"
      },
      "source": [
        "### Classify Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa6D5jfzyNII"
      },
      "source": [
        "# Classify text\n",
        "\n",
        "response = client.classify_text(types.Document(content=text, type=enums.Document.Type.PLAIN_TEXT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXco7s1eyNII"
      },
      "source": [
        "# Print the category name and confidence\n",
        "\n",
        "for category in response.categories:\n",
        "    print(f\"Category name: {category.name}\")\n",
        "    print(f\"Confidence: {round(category.confidence, 2)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szZM13fMyNII"
      },
      "source": [
        "# Find the category name for each review in the list of samples\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "categories = list()\n",
        "\n",
        "for review in df_sample['Review Text']:\n",
        "    document = types.Document(content=review, type=enums.Document.Type.PLAIN_TEXT)\n",
        "    response = client.classify_text(document)\n",
        "    try:\n",
        "        category = response.categories[0].name\n",
        "    except:\n",
        "        category = np.nan\n",
        "    categories.append(category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5qdmI1sLvIm"
      },
      "source": [
        "# Append the category to the review dataframe\n",
        "\n",
        "df_sample = pd.concat([df_sample, pd.DataFrame(categories, columns=['Category'])], axis=1)\n",
        "\n",
        "df_sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFqgjoACyNII"
      },
      "source": [
        "# Plot the count of categories in descending order\n",
        "\n",
        "category_counts = df_sample[['Category','index']].groupby(['Category']).count().rename(columns={'index': 'Count'}).sort_values(by='Count', ascending=False)\n",
        "_ = sns.barplot(x=category_counts['Count'], y=category_counts.index)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
