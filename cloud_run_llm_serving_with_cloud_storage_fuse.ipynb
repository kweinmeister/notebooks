{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTdoL_9FPoo_"
      },
      "source": [
        "# Cloud Run LLM Serving with Cloud Storage FUSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e46f27a"
      },
      "source": [
        "This notebook provides an end-to-end solution for deploying large language models on [Google Cloud Run](https://cloud.google.com/run/docs/), leveraging [Cloud Storage](https://cloud.google.com/storage/docs/) and [Cloud Storage FUSE](https://cloud.google.com/storage/docs/cloud-storage-fuse/overview) for efficient model management.\n",
        "\n",
        "By decoupling model weights from the container image, you can:\n",
        "*   **Deploy models of any size** without being constrained by container image size limits.\n",
        "*   **Rapidly iterate on your application code** without re-building and uploading large model files.\n",
        "*   **Share a single model artifact** across multiple services or applications.\n",
        "\n",
        "This guide walks you through two main stages:\n",
        "1.  A **Cloud Run Job** to download a model from the Hugging Face Hub and store it in a GCS bucket.\n",
        "2.  A **Cloud Run Service** that mounts the GCS bucket using GCS FUSE and serves the model with [Ollama](https://ollama.com/).\n",
        "\n",
        "This approach provides a scalable, cost-effective, and flexible way to serve large models on Google Cloud.\n",
        "\n",
        "### About the Example Model\n",
        "This notebook uses [`unsloth/gemma-3n-E4B-it-GGUF`](https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF) as the example. This is a lightweight, 4-billion parameter multimodal model from Google's Gemma family, optimized by Unsloth for efficient performance. While this guide is ideal for very large models, the principles apply to models of any size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bd31baf"
      },
      "source": [
        "## Section 1: Download Model to GCS with a Cloud Run Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1170ff5"
      },
      "outputs": [],
      "source": [
        "# Install Python packages required for this notebook\n",
        "%pip install --upgrade huggingface_hub hf_transfer --quiet\n",
        "\n",
        "# Specific imports for functionality used directly in sections\n",
        "from google.colab import auth\n",
        "import re\n",
        "\n",
        "print(\"Python environment setup complete. All necessary packages installed and libraries imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fa9d59a"
      },
      "source": [
        "### 1.1. Configure Google Cloud and Notebook Settings\n",
        "This section defines all the necessary parameters for the deployment, including your Google Cloud project, region, and the specifics of the Hugging Face model to be deployed. Resource names for the Cloud Run Job and GCS bucket will now be dynamically generated based on the Hugging Face repository ID for greater flexibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d4fb26c"
      },
      "outputs": [],
      "source": [
        "# @markdown #### **1. Google Cloud Project and Region Configuration**\n",
        "# @markdown Enter your Google Cloud Project ID and desired deployment region.\n",
        "PROJECT_ID = \"your-gcp-project-id\" # @param {type:\"string\"}\n",
        "REGION = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown #### **2. Google Cloud Storage Bucket for Model Storage**\n",
        "# @markdown Enter the name of an existing GCS bucket where the model files will be stored.\n",
        "# @markdown The model files will be placed in a subfolder within this bucket.\n",
        "GCS_BUCKET_NAME = \"your-bucket-name\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown #### **3. Artifact Registry Repository for Docker Images**\n",
        "# @markdown Enter the name of the Artifact Registry repository for your Docker images.\n",
        "ARTIFACT_REGISTRY_REPO = \"docker\" # @param {type:\"string\"}\n",
        "\n",
        "# --- IMPORTANT VALIDATION ---\n",
        "if PROJECT_ID == \"your-gcp-project-id\" or not PROJECT_ID:\n",
        "    print(\"ERROR: Please update 'PROJECT_ID' in this cell with your actual Google Cloud Project ID.\")\n",
        "    print(\"Execution aborted. Please fix the PROJECT_ID and re-run this cell.\")\n",
        "    raise SystemExit(\"PROJECT_ID not set.\")\n",
        "\n",
        "if GCS_BUCKET_NAME == \"your-bucket-name\" or not GCS_BUCKET_NAME:\n",
        "    print(\"ERROR: Please update 'GCS_BUCKET_NAME' in this cell with the name of your GCS bucket.\")\n",
        "    print(\"Execution aborted. Please fix the GCS_BUCKET_NAME and re-run this cell.\")\n",
        "    raise SystemExit(\"GCS_BUCKET_NAME not set.\")\n",
        "# --- END IMPORTANT VALIDATION ---\n",
        "\n",
        "# @markdown #### **2. Cloud Run Job Settings (for Model Transfer)**\n",
        "# @markdown This is the name for the job that copies model files from Hugging Face to GCS.\n",
        "JOB_NAME_SUFFIX = \"-job\"\n",
        "\n",
        "# @markdown #### **3. Hugging Face Model Details**\n",
        "# @markdown Specify the Hugging Face repository and an optional file pattern for the model files.\n",
        "HF_REPO_ID = \"unsloth/gemma-3n-E4B-it-GGUF\" # @param {type:\"string\"}\n",
        "# Cleaned version of HF_REPO_ID for use in resource names\n",
        "HF_REPO_NAME_CLEAN = re.sub(r'[^a-zA-Z0-9]+', '-', HF_REPO_ID).lower()\n",
        "\n",
        "JOB_NAME = f\"{HF_REPO_NAME_CLEAN[:23]}{JOB_NAME_SUFFIX}\"\n",
        "\n",
        "# @markdown CPU and Memory for the transfer job. Increased memory to safely handle larger *metadata* / small files from HF.\n",
        "# @markdown Note: This may require your project to use the Gen2 execution environment in the specified region.\n",
        "JOB_CPU = 2 # @param {type:\"integer\"}\n",
        "JOB_MEMORY_GI = 4 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Optional: Specify a pattern to filter files from the Hugging Face repository (e.g., `*.gguf`, `model.safetensors`).\n",
        "# @markdown Leave blank to download all files.\n",
        "HF_MODEL_FILE_PATTERN = \"*Q4_K_XL*\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Important: Set your Hugging Face API Token in Colab Secrets!**\n",
        "# @markdown To ensure reliable and faster downloads from Hugging Face Hub, it is highly recommended to provide an API token.\n",
        "# @markdown 1. Go to [Hugging Face Settings -> Access Tokens](https://huggingface.co/settings/tokens).\n",
        "# @markdown 2. Create a new token with \"read\" role.\n",
        "# @markdown 3. In Colab, click on the \"ðŸ”‘\" (Secrets) icon on the left sidebar.\n",
        "# @markdown 4. Add a new secret named `HF_TOKEN` and paste your Hugging Face API token as the value.\n",
        "# @markdown 5. Make sure to enable \"Notebook access\" for this secret. The next section will use it to create a Google Cloud Secret.\n",
        "\n",
        "# @markdown #### **5. Cleanup Confirmation**\n",
        "# @markdown Set to `True` to confirm resource deletion in the final cleanup section.\n",
        "CONFIRM_DELETE = False # @param {type:\"boolean\"}\n",
        "\n",
        "# Derived Docker Image Names (using Artifact Registry regional hostname)\n",
        "# The format is REGION-docker.pkg.dev/PROJECT_ID/REPOSITORY_NAME/IMAGE_NAME:TAG\n",
        "JOB_IMAGE_NAME = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REGISTRY_REPO}/{JOB_NAME}:latest\"\n",
        "\n",
        "# Derived Cloud Run Service Accounts\n",
        "JOB_SA_NAME = f\"{JOB_NAME}-sa\"\n",
        "JOB_SA_EMAIL = f\"{JOB_SA_NAME}@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
        "\n",
        "# Define the path within the GCS bucket for the model files (constant)\n",
        "GCS_MODEL_PATH_PREFIX = f\"{HF_REPO_NAME_CLEAN}-model/\" # This remains fixed based on current Kimi K2 structure\n",
        "\n",
        "# Configure `gcloud` CLI with your project and region\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "!gcloud config set run/region {REGION}\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "print(\"Authenticating with Google Cloud. Please follow the instructions in the new browser tab.\")\n",
        "auth.authenticate_user()\n",
        "print(\"Authentication complete.\")\n",
        "\n",
        "# Enable necessary Google Cloud APIs\n",
        "print(\"Enabling required Google Cloud APIs. This may take a moment...\")\n",
        "# secretmanager.googleapis.com is needed for the Cloud Run Job to access HF_TOKEN\n",
        "!gcloud services enable run.googleapis.com artifactregistry.googleapis.com cloudbuild.googleapis.com secretmanager.googleapis.com --project={PROJECT_ID}\n",
        "print(\"Required APIs are enabled.\")\n",
        "\n",
        "print(\"\\nAll configuration and initial cloud setup steps are complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c22ed5f0"
      },
      "source": [
        "### 1.2. Securely Store Hugging Face Token in Google Cloud\n",
        "To ensure your Hugging Face token is handled securely, this step creates a secret in Google Cloud Secret Manager. The Cloud Run job will later access this secret to authenticate with Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01cf30e2"
      },
      "outputs": [],
      "source": [
        "# Load the Hugging Face token from Colab's user secrets\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN_VALUE = userdata.get('HF_TOKEN')\n",
        "    if not HF_TOKEN_VALUE:\n",
        "        raise ValueError(\"HF_TOKEN not found in Colab secrets. Please add it via the 'ðŸ”‘' icon on the left.\")\n",
        "    print(\"Successfully loaded HF_TOKEN from Colab secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not load HF_TOKEN from Colab secrets. Please add it via the 'ðŸ”‘' icon on the left.\")\n",
        "    raise SystemExit(e)\n",
        "\n",
        "# Create the secret in Google Cloud Secret Manager if it doesn't exist\n",
        "# If it exists, this command will fail gracefully.\n",
        "print(\"Attempting to create the 'HF_TOKEN' secret in Google Cloud Secret Manager...\")\n",
        "!gcloud secrets create HF_TOKEN --replication-policy=\"automatic\" --project={PROJECT_ID} --quiet || echo \"Secret 'HF_TOKEN' likely already exists. Continuing.\"\n",
        "\n",
        "# Add the token value as the latest version of the secret. This is idempotent and safe to run multiple times.\n",
        "print(\"Adding the token value as the latest version of the 'HF_TOKEN' secret...\")\n",
        "# The `tr -d '\\n'` removes any trailing newline characters that might interfere with the token.\n",
        "!echo -n \"{HF_TOKEN_VALUE}\" | tr -d '\\n' | gcloud secrets versions add HF_TOKEN --data-file=- --project={PROJECT_ID}\n",
        "\n",
        "print(\"\\nGoogle Cloud Secret 'HF_TOKEN' is now configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fb61ab8"
      },
      "source": [
        "### 1.3. Create Artifact Registry Repository\n",
        "This step ensures that a Docker repository exists in Google Cloud's Artifact Registry. This repository will store the container image for our Cloud Run job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10e12682"
      },
      "outputs": [],
      "source": [
        "print(f\"Ensuring Artifact Registry repository '{ARTIFACT_REGISTRY_REPO}' exists in region '{REGION}'...\")\n",
        "# The `--async` flag prevents the notebook from blocking while the repository is created if it doesn't exist.\n",
        "# It's idempotent, so safe to run even if it exists.\n",
        "!gcloud artifacts repositories create {ARTIFACT_REGISTRY_REPO} \\\n",
        "    --repository-format=docker \\\n",
        "    --location={REGION} \\\n",
        "    --description=\"Docker repository for Hugging Face model images\" \\\n",
        "    --project={PROJECT_ID} \\\n",
        "    --async\n",
        "\n",
        "print(f\"Artifact Registry repository creation command sent. It should be ready shortly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fa59488"
      },
      "source": [
        "### 1.4. Define the Model Transfer Cloud Run Job\n",
        "This section creates the necessary files for our Cloud Run job, which is responsible for transferring the model from Hugging Face to GCS. This includes the Python script, the Dockerfile for containerization, and the build configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpJRB2xKcY8V"
      },
      "outputs": [],
      "source": [
        "%%writefile copy_model_job.py\n",
        "import os\n",
        "import logging\n",
        "import asyncio\n",
        "from urllib.parse import urlparse\n",
        "from huggingface_hub import HfApi, hf_hub_url\n",
        "import obstore as obs\n",
        "from obstore.store import GCSStore, HTTPStore\n",
        "import fnmatch\n",
        "from typing import Dict, Any, Tuple, List\n",
        "\n",
        "# --- Configuration Constants (with Environment Variable Overrides) ---\n",
        "# Maximum number of concurrent file streams.\n",
        "MAX_CONCURRENT_FILES = int(os.getenv(\"MAX_CONCURRENT_FILES\", \"12\"))\n",
        "# Timeout for each individual file download in seconds.\n",
        "INDIVIDUAL_FILE_TIMEOUT_SECONDS = int(os.getenv(\"INDIVIDUAL_FILE_TIMEOUT_SECONDS\", 4 * 60 * 60)) # Default: 4 hours\n",
        "# Log progress every N megabytes.\n",
        "PROGRESS_LOG_INTERVAL_MB = int(os.getenv(\"PROGRESS_LOG_INTERVAL_MB\", \"100\"))\n",
        "\n",
        "# --- Logging Configuration ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "async def progress_stream_wrapper(streaming_response, log_prefix, file_name):\n",
        "    \"\"\"An async generator that wraps a download stream to report progress.\"\"\"\n",
        "    try:\n",
        "        total_size = streaming_response.meta.get(\"size\")\n",
        "        total_size_mb = total_size / (1024 * 1024) if total_size else None\n",
        "    except Exception:\n",
        "        total_size = None\n",
        "        total_size_mb = None\n",
        "\n",
        "    bytes_processed = 0\n",
        "    last_logged_mb = 0\n",
        "    log_interval_bytes = PROGRESS_LOG_INTERVAL_MB * 1024 * 1024\n",
        "\n",
        "    async for chunk in streaming_response:\n",
        "        yield chunk\n",
        "        bytes_processed += len(chunk)\n",
        "\n",
        "        # Log progress periodically\n",
        "        processed_mb = bytes_processed / (1024 * 1024)\n",
        "        if (processed_mb - last_logged_mb) >= PROGRESS_LOG_INTERVAL_MB:\n",
        "            last_logged_mb = processed_mb\n",
        "            if total_size_mb:\n",
        "                percent = (bytes_processed / total_size) * 100\n",
        "                logger.info(\n",
        "                    f\"{log_prefix} -> Progress for '{file_name}': \"\n",
        "                    f\"{processed_mb:.2f} MB / {total_size_mb:.2f} MB ({percent:.1f}%)\"\n",
        "                )\n",
        "            else:\n",
        "                logger.info(\n",
        "                    f\"{log_prefix} -> Progress for '{file_name}': {processed_mb:.2f} MB\"\n",
        "                )\n",
        "\n",
        "\n",
        "async def stream_file_to_gcs(\n",
        "    file_name: str,\n",
        "    config: Dict[str, Any],\n",
        "    stores: Tuple[GCSStore, HTTPStore],\n",
        "    semaphore: asyncio.Semaphore,\n",
        "    file_index: int,\n",
        "    total_files: int,\n",
        "):\n",
        "    \"\"\"Streams a file from a Hugging Face repo directly to GCS using obstore.\"\"\"\n",
        "    gcs_store, http_store = stores\n",
        "    gcs_destination_path = os.path.join(config[\"gcs_path_prefix\"], file_name)\n",
        "    log_prefix = f\"[{file_index + 1}/{total_files}]\"\n",
        "\n",
        "    async with semaphore:\n",
        "        logger.info(\n",
        "            f\"{log_prefix} Starting stream of '{file_name}' to \"\n",
        "            f\"GCS: gs://{config['gcs_bucket_name']}/{gcs_destination_path}\"\n",
        "        )\n",
        "        try:\n",
        "            full_download_url = hf_hub_url(\n",
        "                repo_id=config[\"hf_repo_id\"], filename=file_name, revision=\"main\"\n",
        "            )\n",
        "            download_path = urlparse(full_download_url).path\n",
        "\n",
        "            streaming_response = await obs.get_async(http_store, download_path)\n",
        "\n",
        "            progress_stream = progress_stream_wrapper(\n",
        "                streaming_response, log_prefix, file_name\n",
        "            )\n",
        "\n",
        "            await obs.put_async(gcs_store, gcs_destination_path, progress_stream)\n",
        "\n",
        "            logger.info(\n",
        "                f\"{log_prefix} Successfully streamed '{file_name}' to GCS.\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(\n",
        "                f\"{log_prefix} FATAL: Error processing '{file_name}': {e}\",\n",
        "                exc_info=True,\n",
        "            )\n",
        "            # Re-raising the exception will cause asyncio.gather to fail.\n",
        "            raise\n",
        "\n",
        "\n",
        "def _get_job_config() -> Dict[str, Any]:\n",
        "    \"\"\"Reads and validates all configuration from environment variables.\"\"\"\n",
        "    logger.info(\"Verifying environment variables...\")\n",
        "    config = {\n",
        "        \"hf_repo_id\": os.getenv(\"HF_REPO_ID\"),\n",
        "        \"hf_model_file_pattern\": os.getenv(\"HF_MODEL_FILE_PATTERN\"), # Optional\n",
        "        \"gcs_bucket_name\": os.getenv(\"GCS_BUCKET_NAME\"),\n",
        "        \"gcs_path_prefix\": os.getenv(\"GCS_MODEL_PATH_PREFIX\"),\n",
        "        \"hf_token\": None\n",
        "    }\n",
        "\n",
        "    hf_token_file_path = \"/etc/secrets/hf-token/HF_TOKEN\"\n",
        "    if os.path.exists(hf_token_file_path):\n",
        "        try:\n",
        "            with open(hf_token_file_path, \"r\") as f:\n",
        "                config[\"hf_token\"] = f.read().strip()\n",
        "                logger.info(\"Hugging Face token loaded successfully from mounted secret.\")\n",
        "        except Exception as e:\n",
        "            raise IOError(f\"Error reading HF_TOKEN from mounted file: {e}\") from e\n",
        "    else:\n",
        "        logger.warning(\n",
        "            f\"HF_TOKEN file not found at {hf_token_file_path}. \"\n",
        "            f\"Downloads will be unauthenticated and might be slower or fail.\"\n",
        "        )\n",
        "\n",
        "    required_vars = [\"hf_repo_id\", \"gcs_bucket_name\", \"gcs_path_prefix\"]\n",
        "    if not all(config[key] for key in required_vars):\n",
        "        raise ValueError(\"Missing one or more required environment variables: \"\n",
        "                         \"HF_REPO_ID, GCS_BUCKET_NAME, GCS_MODEL_PATH_PREFIX.\")\n",
        "\n",
        "    logger.info(\"All required environment variables are set.\")\n",
        "    return config\n",
        "\n",
        "\n",
        "def _initialize_clients(config: Dict[str, Any]) -> Tuple[GCSStore, HTTPStore]:\n",
        "    \"\"\"Initializes and returns HTTP and GCS stores.\"\"\"\n",
        "    logger.info(\"Initializing obstore HTTP and GCS stores...\")\n",
        "    try:\n",
        "        # Explicitly build the client_options dictionary ensuring all values\n",
        "        # are in a format the Rust layer can handle directly (strings).\n",
        "        client_options: Dict[str, Any] = {\n",
        "            # Convert the integer timeout to a \"human-readable duration string\"\n",
        "            \"timeout\": f\"{INDIVIDUAL_FILE_TIMEOUT_SECONDS}s\",\n",
        "        }\n",
        "        if config[\"hf_token\"]:\n",
        "            # The correct key for headers is 'default_headers'.\n",
        "            client_options[\"default_headers\"] = {\n",
        "                \"Authorization\": f\"Bearer {config['hf_token']}\"\n",
        "            }\n",
        "\n",
        "        # Pass the fully constructed dictionary to the 'client_options' argument.\n",
        "        http_store = HTTPStore.from_url(\n",
        "            \"https://huggingface.co\", client_options=client_options\n",
        "        )\n",
        "\n",
        "        gcs_store = GCSStore(bucket=config[\"gcs_bucket_name\"])\n",
        "        logger.info(\n",
        "            \"Successfully initialized stores for Hugging Face and GCS bucket: \"\n",
        "            f\"{config['gcs_bucket_name']}\"\n",
        "        )\n",
        "        return gcs_store, http_store\n",
        "    except Exception as e:\n",
        "        raise ConnectionError(f\"Failed to initialize obstore clients: {e}\") from e\n",
        "\n",
        "\n",
        "def _get_target_files(api: HfApi, config: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"Lists files from the repo and filters them based on the pattern.\"\"\"\n",
        "    repo_id = config[\"hf_repo_id\"]\n",
        "    pattern = config[\"hf_model_file_pattern\"]\n",
        "    logger.info(f\"Listing files in Hugging Face repo '{repo_id}'...\")\n",
        "    try:\n",
        "        files_info = api.list_repo_files(repo_id=repo_id, token=config[\"hf_token\"])\n",
        "        logger.info(f\"Successfully listed {len(files_info)} files from repo '{repo_id}'.\")\n",
        "    except Exception as e:\n",
        "        raise ConnectionError(f\"Failed to list files from repo '{repo_id}': {e}\") from e\n",
        "\n",
        "    if pattern:\n",
        "        return [f for f in files_info if fnmatch.fnmatch(f, pattern)]\n",
        "    return files_info\n",
        "\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main execution function for the Cloud Run Job.\"\"\"\n",
        "    logger.info(\"Cloud Run Job started.\")\n",
        "    try:\n",
        "        # 1. Setup and Configuration\n",
        "        config = _get_job_config()\n",
        "        stores = _initialize_clients(config)\n",
        "        api = HfApi()\n",
        "\n",
        "        # 2. Get the list of files to transfer\n",
        "        target_files = _get_target_files(api, config)\n",
        "\n",
        "        if not target_files:\n",
        "            pattern = config[\"hf_model_file_pattern\"]\n",
        "            logger.warning(\n",
        "                f\"No files found matching pattern '{pattern}' in repo \"\n",
        "                f\"'{config['hf_repo_id']}'. Job will complete successfully.\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        pattern_info = (f\"matching pattern '{config['hf_model_file_pattern']}'\"\n",
        "                        if config['hf_model_file_pattern'] else \"in repo\")\n",
        "        logger.info(\n",
        "            f\"Found {len(target_files)} files {pattern_info}. \"\n",
        "            f\"Proceeding to download and upload concurrently.\"\n",
        "        )\n",
        "\n",
        "        # 3. Create and run concurrent download/upload tasks\n",
        "        semaphore = asyncio.Semaphore(MAX_CONCURRENT_FILES)\n",
        "        logger.info(f\"Limiting concurrent files to {MAX_CONCURRENT_FILES} at a time.\")\n",
        "\n",
        "        tasks = [\n",
        "            stream_file_to_gcs(\n",
        "                file_name=file_name,\n",
        "                config=config,\n",
        "                stores=stores,\n",
        "                semaphore=semaphore,\n",
        "                file_index=i,\n",
        "                total_files=len(target_files),\n",
        "            )\n",
        "            for i, file_name in enumerate(target_files)\n",
        "        ]\n",
        "\n",
        "        await asyncio.gather(*tasks, return_exceptions=False)\n",
        "\n",
        "        logger.info(\n",
        "            \"All model files have been processed and are now in GCS. \"\n",
        "            \"Job completed successfully.\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"FATAL: Job failed during execution: {e}\", exc_info=True)\n",
        "        exit(1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAXcjSjQceh3"
      },
      "outputs": [],
      "source": [
        "%%writefile Dockerfile\n",
        "FROM python:3.13-slim\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "COPY copy_model_job.py .\n",
        "\n",
        "CMD [\"python\", \"copy_model_job.py\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TezUOmdrcgxF"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "google-cloud-storage\n",
        "huggingface-hub\n",
        "obstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXZVLpxbc8hq"
      },
      "outputs": [],
      "source": [
        "%%writefile .gcloudignore\n",
        "*\n",
        "\n",
        "!copy_model_job.py\n",
        "!Dockerfile\n",
        "!requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtQPHl9tc4Ky"
      },
      "outputs": [],
      "source": [
        "CLOUDBUILD_JOB_YAML_CONTENT = f\"\"\"\n",
        "steps:\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: ['build', '-t', '{JOB_IMAGE_NAME}', '-f', 'Dockerfile', '.']\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: ['push', '{JOB_IMAGE_NAME}']\n",
        "images:\n",
        "- {JOB_IMAGE_NAME}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"cloudbuild.job.yaml\", \"w\") as f:\n",
        "    f.write(CLOUDBUILD_JOB_YAML_CONTENT)\n",
        "\n",
        "print(\"cloudbuild.job.yaml created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "905a7df8"
      },
      "source": [
        "### 1.5. Build and Execute the Cloud Run Job\n",
        "This section handles the final steps of the deployment process. It creates a dedicated service account for the job, assigns the necessary permissions, builds the Docker image using Cloud Build, and then deploys and runs the job on Cloud Run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHe633Yr_Tor"
      },
      "outputs": [],
      "source": [
        "# Create a service account for the Cloud Run Job\n",
        "print(f\"Creating service account for Cloud Run Job: {JOB_SA_EMAIL}...\")\n",
        "!gcloud iam service-accounts create {JOB_SA_NAME} \\\n",
        "    --display-name=\"{HF_REPO_ID} Model Transfer Job SA\" \\\n",
        "    --project={PROJECT_ID} --quiet\n",
        "\n",
        "print(\"Granting necessary permissions to the Cloud Run Job service account...\")\n",
        "# Grant Storage Object Admin role, which includes create, get, list, and delete permissions for objects.\n",
        "!gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "    --member=\"serviceAccount:{JOB_SA_EMAIL}\" \\\n",
        "    --role=\"roles/storage.objectAdmin\" \\\n",
        "    --condition=None --quiet > /dev/null\n",
        "\n",
        "# Grant Secret Manager Secret Accessor role for HF_TOKEN\n",
        "print(\"Granting Secret Manager Secret Accessor role to the job service account...\")\n",
        "!gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "    --member=\"serviceAccount:{JOB_SA_EMAIL}\" \\\n",
        "    --role=\"roles/secretmanager.secretAccessor\" \\\n",
        "    --condition=None --quiet > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc512967"
      },
      "outputs": [],
      "source": [
        "# Build the Docker image for the Cloud Run Job using Cloud Build\n",
        "print(f\"Submitting build job for Cloud Run Job image: {JOB_IMAGE_NAME}. This may take a few minutes.\")\n",
        "# The `.` at the end will now respect the .gcloudignore file\n",
        "!gcloud builds submit --config cloudbuild.job.yaml --project={PROJECT_ID} ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU-luJ5h_Naa"
      },
      "outputs": [],
      "source": [
        "!gcloud beta run jobs deploy {JOB_NAME} \\\n",
        "    --image {JOB_IMAGE_NAME} \\\n",
        "    --region {REGION} \\\n",
        "    --cpu {JOB_CPU} \\\n",
        "    --memory {JOB_MEMORY_GI}Gi \\\n",
        "    --service-account {JOB_SA_EMAIL} \\\n",
        "    --labels dev-tutorial=notebook-gcsfuse \\\n",
        "    --set-env-vars HF_REPO_ID={HF_REPO_ID},HF_MODEL_FILE_PATTERN={HF_MODEL_FILE_PATTERN},GCS_BUCKET_NAME={GCS_BUCKET_NAME},GCS_MODEL_PATH_PREFIX={GCS_MODEL_PATH_PREFIX} \\\n",
        "    --set-secrets=/etc/secrets/hf-token/HF_TOKEN=HF_TOKEN:latest \\\n",
        "    --project={PROJECT_ID} \\\n",
        "    --task-timeout 24h \\\n",
        "    --execute-now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "014df360"
      },
      "source": [
        "## Section 2: Host the Model with a Cloud Run Service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "499c6663"
      },
      "source": [
        "### 2.1. Create the Service Configuration Files\n",
        "\n",
        "This next section creates the configuration files for the Cloud Run service. These files tell the service how to load and serve the model.\n",
        "\n",
        "*   **`start.sh`**: This is the entrypoint script for the container. It performs the following steps:\n",
        "    1.  Calculates the SHA256 hash of the model file.\n",
        "    2.  Creates a symbolic link from the model file to the Ollama blobs directory. This is a crucial step that allows Ollama to discover and use the model from the GCS FUSE mount.\n",
        "    3.  Creates the model manifest using the `ollama create` command.\n",
        "    4.  Starts the Ollama server.\n",
        "*   **`Modelfile`**: This file defines the model that Ollama will serve. It simply points to the model file that is mounted from GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0Tq_oPGgC38"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Construct the full GCS path including the model subfolder and file pattern\n",
        "gcs_path_pattern = f\"gs://{GCS_BUCKET_NAME}/{GCS_MODEL_PATH_PREFIX}{HF_MODEL_FILE_PATTERN}\"\n",
        "\n",
        "print(f\"Searching for model files in GCS at: {gcs_path_pattern}\")\n",
        "\n",
        "# Execute the gsutil ls command and capture the output\n",
        "# The '2>/dev/null' part suppresses potential error messages if no files are found\n",
        "gsutil_output = !gsutil ls {gcs_path_pattern} 2>/dev/null\n",
        "\n",
        "# --- Validation ---\n",
        "if not gsutil_output:\n",
        "    print(f\"ERROR: No files found in GCS matching the pattern.\")\n",
        "    print(\"Please ensure the model transfer job has completed successfully and the GCS_BUCKET_NAME, GCS_MODEL_PATH_PREFIX, and HF_MODEL_FILE_PATTERN variables are correct.\")\n",
        "    sys.exit(\"Aborting due to missing model file in GCS.\")\n",
        "\n",
        "# Take the first file from the list (gsutil ls sorts alphabetically)\n",
        "first_file_full_path = gsutil_output[0]\n",
        "\n",
        "# Extract just the filename from the full gs:// path\n",
        "MODEL_FILENAME_FROM_GCS = os.path.basename(first_file_full_path)\n",
        "\n",
        "print(f\"Successfully identified model file in GCS: {MODEL_FILENAME_FROM_GCS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaIM-5RvCTxQ"
      },
      "outputs": [],
      "source": [
        "!mkdir ollama_service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vN_BERqj3D4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "gcs_path_pattern = f\"gs://{GCS_BUCKET_NAME}/{GCS_MODEL_PATH_PREFIX}{HF_MODEL_FILE_PATTERN}\"\n",
        "print(f\"Searching for model file in GCS: {gcs_path_pattern}\")\n",
        "\n",
        "gsutil_output = !gsutil ls {gcs_path_pattern} 2>/dev/null\n",
        "if not gsutil_output:\n",
        "    print(\"ERROR: No files found in GCS matching the pattern. Ensure the transfer job is complete.\")\n",
        "    sys.exit(\"Aborting.\")\n",
        "\n",
        "MODEL_GCS_PATH = gsutil_output[0]\n",
        "MODEL_FILENAME = os.path.basename(MODEL_GCS_PATH)\n",
        "print(f\"Successfully identified model file in GCS: {MODEL_GCS_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoD1w1a1kOCa"
      },
      "outputs": [],
      "source": [
        "# This is the full, absolute path to the model file *inside the container*.\n",
        "full_model_path_in_container = f\"/models/{GCS_MODEL_PATH_PREFIX}{MODEL_FILENAME}\"\n",
        "\n",
        "# --- Create start.sh Script Content ---\n",
        "# The f-string now starts IMMEDIATELY with #!/bin/sh to avoid any\n",
        "# leading newlines. This is the critical fix.\n",
        "START_SH_CONTENT = f\"\"\"#!/bin/sh\n",
        "\n",
        "# This path was dynamically inserted by the deployment notebook.\n",
        "MODEL_FILE_PATH=\"{full_model_path_in_container}\"\n",
        "MODEL_NAME=\"gemma-3n-custom\"\n",
        "BLOBS_DIR=\"/var/lib/ollama/blobs\"\n",
        "\n",
        "echo \"Starting Ollama server in the background...\"\n",
        "ollama serve &\n",
        "sleep 3\n",
        "\n",
        "echo \"Calculating SHA256 for $MODEL_FILE_PATH...\"\n",
        "MODEL_SHA256=$(sha256sum \"$MODEL_FILE_PATH\" | awk '{{print $1}}')\n",
        "\n",
        "if [ -z \"$MODEL_SHA256\" ]; then\n",
        "    echo \"ERROR: Failed to calculate SHA256 hash for the model file.\"\n",
        "    exit 1\n",
        "fi\n",
        "echo \"SHA256 calculated: $MODEL_SHA256\"\n",
        "\n",
        "BLOB_PATH=\"$BLOBS_DIR/sha256-$MODEL_SHA256\"\n",
        "\n",
        "echo \"Checking if blob already exists or needs a symlink...\"\n",
        "if [ ! -e \"$BLOB_PATH\" ]; then\n",
        "    echo \"Creating symlink from $MODEL_FILE_PATH to $BLOB_PATH\"\n",
        "    mkdir -p \"$BLOBS_DIR\"\n",
        "    ln -s \"$MODEL_FILE_PATH\" \"$BLOB_PATH\"\n",
        "else\n",
        "    echo \"Blob path already exists. No symlink needed.\"\n",
        "fi\n",
        "\n",
        "echo \"Running 'ollama create' to generate the manifest...\"\n",
        "ollama create \"$MODEL_NAME\" -f /workspace/Modelfile\n",
        "\n",
        "echo \"Model created. Bringing Ollama to the foreground.\"\n",
        "wait $!\n",
        "\"\"\"\n",
        "\n",
        "# Write the dynamic start.sh file\n",
        "with open(\"ollama_service/start.sh\", \"w\") as f:\n",
        "    f.write(START_SH_CONTENT)\n",
        "print(\"Successfully created dynamic ollama_service/start.sh with correct formatting.\")\n",
        "\n",
        "\n",
        "# --- Create Modelfile Content (This part was correct) ---\n",
        "MODELFILE_CONTENT = f\"\"\"\n",
        "# This Modelfile was dynamically generated to point to the correct model file.\n",
        "FROM {full_model_path_in_container}\n",
        "\"\"\"\n",
        "\n",
        "# Write the dynamic Modelfile\n",
        "with open(\"ollama_service/Modelfile\", \"w\") as f:\n",
        "    f.write(MODELFILE_CONTENT)\n",
        "print(\"Successfully created dynamic ollama_service/Modelfile\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeHIRtrUCXs3"
      },
      "outputs": [],
      "source": [
        "%%writefile ollama_service/Dockerfile\n",
        "\n",
        "FROM ollama/ollama:latest\n",
        "\n",
        "# Set environment variables\n",
        "ENV OLLAMA_HOST=0.0.0.0:8080\n",
        "ENV OLLAMA_MODELS=/var/lib/ollama\n",
        "ENV OLLAMA_DEBUG=false\n",
        "ENV OLLAMA_KEEP_ALIVE=-1\n",
        "\n",
        "# Copy the Modelfile and our startup script into the container's workspace\n",
        "COPY Modelfile start.sh /workspace/\n",
        "\n",
        "# Make our startup script executable\n",
        "RUN chmod +x /workspace/start.sh\n",
        "\n",
        "# Set the entrypoint to our script. This will run when the container starts.\n",
        "ENTRYPOINT [ \"/workspace/start.sh\" ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4983b11f"
      },
      "source": [
        "### 2.2. Deploy the Service to Cloud Run\n",
        "\n",
        "This final step deploys the service to Cloud Run. The following flags are particularly important:\n",
        "\n",
        "*   `--add-volume=name=ollama-gcs-models,type=cloud-storage,bucket={GCS_BUCKET_NAME},readonly=true`: This flag mounts the GCS bucket containing the model into the container as a read-only volume.\n",
        "*   `--add-volume-mount=volume=ollama-gcs-models,mount-path=/models`: This flag mounts the GCS volume at the `/models` path in the container.\n",
        "*   `--add-volume=name=ollama-writable-state,type=in-memory,size-limit=1Gi`: This flag creates a writable in-memory volume for Ollama's state. This is necessary because the GCS volume is read-only, but Ollama needs a writable directory to store its state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqfRY0iACdp_"
      },
      "outputs": [],
      "source": [
        "!gcloud run deploy {HF_REPO_NAME_CLEAN[:63]} \\\n",
        "    --source ./ollama_service \\\n",
        "    --region {REGION} \\\n",
        "    --labels dev-tutorial=notebook-gcsfuse \\\n",
        "    --set-env-vars OLLAMA_NUM_PARALLEL=4 \\\n",
        "    --project {PROJECT_ID} \\\n",
        "    --gpu 1 \\\n",
        "    --gpu-type nvidia-l4 \\\n",
        "    --max-instances 1 \\\n",
        "    --memory 32Gi \\\n",
        "    --cpu 8 \\\n",
        "    --concurrency 4 \\\n",
        "    --timeout=600 \\\n",
        "    --no-cpu-throttling \\\n",
        "    --allow-unauthenticated \\\n",
        "    --execution-environment=gen2 \\\n",
        "    --no-gpu-zonal-redundancy \\\n",
        "    --add-volume=name=ollama-gcs-models,type=cloud-storage,bucket={GCS_BUCKET_NAME},readonly=true \\\n",
        "    --add-volume-mount=volume=ollama-gcs-models,mount-path=/models \\\n",
        "    --add-volume=name=ollama-writable-state,type=in-memory,size-limit=1Gi \\\n",
        "    --add-volume-mount=volume=ollama-writable-state,mount-path=/var/lib/ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36dfbf1e"
      },
      "source": [
        "### 2.3. Run Inference on the Deployed Model\n",
        "\n",
        "Now that the service is deployed, this step sends a test prompt to the model's API endpoint. It constructs a `curl` command to make a POST request with a JSON payload containing the prompt. The response from the model is then parsed and displayed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMNYV6UHhm6K"
      },
      "outputs": [],
      "source": [
        "# Get the URL of our newly deployed Cloud Run service\n",
        "SERVICE_URL = !gcloud run services describe {HF_REPO_NAME_CLEAN[:63]} --platform managed --region {REGION} --format 'value(status.url)'\n",
        "SERVICE_URL = SERVICE_URL[0]\n",
        "\n",
        "# The /api/generate endpoint is the standard path for Ollama prompts\n",
        "OLLAMA_ENDPOINT_URL = f\"{SERVICE_URL}/api/generate\"\n",
        "\n",
        "print(f\"Service is running. Endpoint URL for prompts is:\\n{OLLAMA_ENDPOINT_URL}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s -X POST -H \"Content-Type: application/json\" -d {json_payload} {OLLAMA_ENDPOINT_URL}\n"
      ],
      "metadata": {
        "id": "qjHBmnUxreo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddr_1Av8imJz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Define the service endpoint URL (assuming SERVICE_URL is already defined)\n",
        "OLLAMA_ENDPOINT_URL = f\"{SERVICE_URL}/api/generate\"\n",
        "\n",
        "# Define the data payload for the prompt as a Python dictionary\n",
        "prompt_data = {\n",
        "  \"model\": \"gemma-3n-custom:latest\",\n",
        "  \"prompt\": \"What are the top 3 benefits of accessing models from a GCS bucket instead of the container image?\",\n",
        "  \"stream\": False\n",
        "}\n",
        "\n",
        "# Convert the Python dictionary to a JSON string for the curl command\n",
        "# The single quotes around the f-string are important for the shell command\n",
        "json_payload = f\"'{json.dumps(prompt_data)}'\"\n",
        "\n",
        "# 1. Execute the curl command and capture its output into the 'raw_output' list\n",
        "raw_output = !curl -s -X POST -H \"Content-Type: application/json\" -d {json_payload} {OLLAMA_ENDPOINT_URL}\n",
        "\n",
        "# 2. The output is a list of lines; join them into a single string\n",
        "response_string = \"\".join(raw_output)\n",
        "\n",
        "# 3. Parse the JSON string into a Python dictionary\n",
        "response_json = json.loads(response_string)\n",
        "\n",
        "# 4. Extract and print just the 'response' key\n",
        "model_answer = response_json.get(\"response\", \"Error: 'response' key not found in the JSON output.\")\n",
        "print(model_answer.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stHtCseO5mEm"
      },
      "source": [
        "## Optional: Clean Up Resources\n",
        "\n",
        "This final section provides commands to delete all the Google Cloud resources created during this guide. It's crucial to run this step when you are finished to avoid incurring unnecessary costs, especially for GPU resources and large storage buckets.\n",
        "\n",
        "**WARNING**: Running this section will delete your Cloud Run service, the Cloud Run Job, Docker image repositories in Artifact Registry, and your Cloud Storage bucket containing the model. This action is irreversible. Ensure `CONFIRM_DELETE` is set to `True` in the **Google Cloud Configuration** section to enable this cleanup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93DUxTXN5ldi"
      },
      "outputs": [],
      "source": [
        "if CONFIRM_DELETE:\n",
        "    print(\"Initiating cleanup of Google Cloud resources...\")\n",
        "\n",
        "    # Delete the Cloud Run Job\n",
        "    print(f\"Deleting Cloud Run Job '{JOB_NAME}'...\")\n",
        "    !gcloud run jobs delete {JOB_NAME} --region={REGION} --quiet --project={PROJECT_ID}\n",
        "\n",
        "    # Delete the Docker image repositories from Artifact Registry\n",
        "    # This deletes the entire 'docker' repository in your chosen REGION for the project.\n",
        "    print(f\"Deleting Docker image repository 'docker' in region '{REGION}'...\")\n",
        "    !gcloud artifacts repositories delete docker --location={REGION} --project={PROJECT_ID} --quiet || true\n",
        "\n",
        "    # Delete the Cloud Storage bucket (and all its contents)\n",
        "    print(f\"Deleting Cloud Storage bucket 'gs://{GCS_BUCKET_NAME}'...\")\n",
        "    !gsutil -r rm -r gs://{GCS_BUCKET_NAME}\n",
        "\n",
        "    # Delete the Secret Manager secret for HF_TOKEN\n",
        "    print(f\"Deleting Secret Manager secret 'HF_TOKEN'...\")\n",
        "    !gcloud secrets delete HF_TOKEN --project={PROJECT_ID} --quiet || true\n",
        "\n",
        "    # Delete the custom service accounts\n",
        "    print(f\"Deleting job service account '{JOB_SA_EMAIL}'...\")\n",
        "    !gcloud iam service-accounts delete {JOB_SA_EMAIL} --project={PROJECT_ID} --quiet --display-name=\"{HF_REPO_ID} Model Transfer Job SA\" || true\n",
        "\n",
        "    print(\"\\nCleanup commands executed. Please verify in the Google Cloud Console that all resources have been removed to prevent further charges.\")\n",
        "else:\n",
        "    print(\"Resource deletion not confirmed. To delete resources, set `CONFIRM_DELETE = True` in the Google Cloud Configuration section and re-run this section.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.12.4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}